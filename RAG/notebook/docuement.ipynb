{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "464d13dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a5a1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Krish Naik', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc = Document(\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Krish Naik\",\n",
    "        \"date_created\": \"2025-01-01\"\n",
    "    },\n",
    "    page_content=\"this is the main text content I am using to create RAG\"\n",
    ")\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7b1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os. makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e132c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"../data/text_files/finadapt_intro.txt\": \"\"\"\n",
    "FinAdapt – India’s First Agentic-AI Powered Financial Guidance System for Gig Workers\n",
    "\n",
    "FinAdapt is a next-generation financial empowerment platform designed exclusively for India’s rapidly growing gig economy. \n",
    "In a world where freelancers, delivery partners, cab drivers, creators, tutors, technicians, and part-time earners struggle \n",
    "with irregular income and zero traditional financial support, FinAdapt acts as a dedicated, intelligent financial mentor.\n",
    "\n",
    "Unlike typical budgeting apps that only track expenses, FinAdapt uses agentic AI––a system capable of autonomous reasoning, \n",
    "planning, and multi-step decision making––to understand the user’s income patterns, financial habits, and long-term goals. \n",
    "It then creates real-time, personalized strategies for savings, investments, expense management, loans, taxes, and \n",
    "overall financial health.\n",
    "\n",
    "FinAdapt is built on the belief that **gig workers deserve the same financial clarity, security, and opportunity as \n",
    "full-time employees**, even if their income is unstable.\n",
    "\n",
    "Key Capabilities of FinAdapt:\n",
    "- **Dynamic budgeting engine** that adapts automatically to fluctuating income levels every week\n",
    "- **AI-generated smart savings goals** for essentials, emergencies, and personal aspirations\n",
    "- **Cashflow forecasting** that predicts upcoming shortages or surplus using the user’s historical spending behavior\n",
    "- **Automated tax assistant** that calculates tax liability, eligible deductions, and compliance tasks for gig workers\n",
    "- **Intelligent expense categorization** using AI-based receipt reading and transaction analysis\n",
    "- **Personalized loan readiness score** to help gig workers understand eligibility and reduce financial risk\n",
    "- **Insurance guidance** that explains health, life, and accident coverage in extremely simple words\n",
    "- **Educational micro-modules** that teach users financial literacy in easy regional-language content\n",
    "- **Real-time agentic AI mentor** that can plan, prioritize, compare, calculate, and recommend financial decisions like a human advisor\n",
    "- **Goal-based financial planning** for buying a bike, phone, home, education, or starting a business\n",
    "\n",
    "Vision and Impact:\n",
    "FinAdapt aims to become the financial backbone of India’s 100+ million gig and informal workers. The platform is not \n",
    "just a tool—it is a safety net, a support system, and a long-term financial growth partner. By combining agentic AI with \n",
    "deep financial workflows, FinAdapt ensures that every gig worker gains access to structured financial planning without \n",
    "needing prior knowledge or stable monthly income.\n",
    "\n",
    "With FinAdapt, gig workers finally receive:\n",
    "- More control over cashflow\n",
    "- Better financial decisions\n",
    "- Reduced stress during low-income months\n",
    "- Improved long-term financial stability\n",
    "- A stronger path toward savings, investments, and creditworthiness\n",
    "\n",
    "FinAdapt isn’t just another fintech project—it is a mission to build financial confidence, bridge opportunity gaps, \n",
    "and bring financial dignity to millions of independent workers across the country.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for filepath, content in sample_texts.items():\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81f1388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Documents:\n",
      "[Document(metadata={'source': '../data/text_files/finadapt_intro.txt'}, page_content='\\nFinAdapt – India’s First Agentic-AI Powered Financial Guidance System for Gig Workers\\n\\nFinAdapt is a next-generation financial empowerment platform designed exclusively for India’s rapidly growing gig economy. \\nIn a world where freelancers, delivery partners, cab drivers, creators, tutors, technicians, and part-time earners struggle \\nwith irregular income and zero traditional financial support, FinAdapt acts as a dedicated, intelligent financial mentor.\\n\\nUnlike typical budgeting apps that only track expenses, FinAdapt uses agentic AI––a system capable of autonomous reasoning, \\nplanning, and multi-step decision making––to understand the user’s income patterns, financial habits, and long-term goals. \\nIt then creates real-time, personalized strategies for savings, investments, expense management, loans, taxes, and \\noverall financial health.\\n\\nFinAdapt is built on the belief that **gig workers deserve the same financial clarity, security, and opportunity as \\nfull-time employees**, even if their income is unstable.\\n\\nKey Capabilities of FinAdapt:\\n- **Dynamic budgeting engine** that adapts automatically to fluctuating income levels every week\\n- **AI-generated smart savings goals** for essentials, emergencies, and personal aspirations\\n- **Cashflow forecasting** that predicts upcoming shortages or surplus using the user’s historical spending behavior\\n- **Automated tax assistant** that calculates tax liability, eligible deductions, and compliance tasks for gig workers\\n- **Intelligent expense categorization** using AI-based receipt reading and transaction analysis\\n- **Personalized loan readiness score** to help gig workers understand eligibility and reduce financial risk\\n- **Insurance guidance** that explains health, life, and accident coverage in extremely simple words\\n- **Educational micro-modules** that teach users financial literacy in easy regional-language content\\n- **Real-time agentic AI mentor** that can plan, prioritize, compare, calculate, and recommend financial decisions like a human advisor\\n- **Goal-based financial planning** for buying a bike, phone, home, education, or starting a business\\n\\nVision and Impact:\\nFinAdapt aims to become the financial backbone of India’s 100+ million gig and informal workers. The platform is not \\njust a tool—it is a safety net, a support system, and a long-term financial growth partner. By combining agentic AI with \\ndeep financial workflows, FinAdapt ensures that every gig worker gains access to structured financial planning without \\nneeding prior knowledge or stable monthly income.\\n\\nWith FinAdapt, gig workers finally receive:\\n- More control over cashflow\\n- Better financial decisions\\n- Reduced stress during low-income months\\n- Improved long-term financial stability\\n- A stronger path toward savings, investments, and creditworthiness\\n\\nFinAdapt isn’t just another fintech project—it is a mission to build financial confidence, bridge opportunity gaps, \\nand bring financial dignity to millions of independent workers across the country.\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"../data/text_files/finadapt_intro.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Loaded Documents:\")\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425e8788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents:\n",
      "[Document(metadata={'source': '../data/text_files/finadapt_intro.txt'}, page_content='\\nFinAdapt – India’s First Agentic-AI Powered Financial Guidance System for Gig Workers\\n\\nFinAdapt is a next-generation financial empowerment platform designed exclusively for India’s rapidly growing gig economy. \\nIn a world where freelancers, delivery partners, cab drivers, creators, tutors, technicians, and part-time earners struggle \\nwith irregular income and zero traditional financial support, FinAdapt acts as a dedicated, intelligent financial mentor.\\n\\nUnlike typical budgeting apps that only track expenses, FinAdapt uses agentic AI––a system capable of autonomous reasoning, \\nplanning, and multi-step decision making––to understand the user’s income patterns, financial habits, and long-term goals. \\nIt then creates real-time, personalized strategies for savings, investments, expense management, loans, taxes, and \\noverall financial health.\\n\\nFinAdapt is built on the belief that **gig workers deserve the same financial clarity, security, and opportunity as \\nfull-time employees**, even if their income is unstable.\\n\\nKey Capabilities of FinAdapt:\\n- **Dynamic budgeting engine** that adapts automatically to fluctuating income levels every week\\n- **AI-generated smart savings goals** for essentials, emergencies, and personal aspirations\\n- **Cashflow forecasting** that predicts upcoming shortages or surplus using the user’s historical spending behavior\\n- **Automated tax assistant** that calculates tax liability, eligible deductions, and compliance tasks for gig workers\\n- **Intelligent expense categorization** using AI-based receipt reading and transaction analysis\\n- **Personalized loan readiness score** to help gig workers understand eligibility and reduce financial risk\\n- **Insurance guidance** that explains health, life, and accident coverage in extremely simple words\\n- **Educational micro-modules** that teach users financial literacy in easy regional-language content\\n- **Real-time agentic AI mentor** that can plan, prioritize, compare, calculate, and recommend financial decisions like a human advisor\\n- **Goal-based financial planning** for buying a bike, phone, home, education, or starting a business\\n\\nVision and Impact:\\nFinAdapt aims to become the financial backbone of India’s 100+ million gig and informal workers. The platform is not \\njust a tool—it is a safety net, a support system, and a long-term financial growth partner. By combining agentic AI with \\ndeep financial workflows, FinAdapt ensures that every gig worker gains access to structured financial planning without \\nneeding prior knowledge or stable monthly income.\\n\\nWith FinAdapt, gig workers finally receive:\\n- More control over cashflow\\n- Better financial decisions\\n- Reduced stress during low-income months\\n- Improved long-term financial stability\\n- A stronger path toward savings, investments, and creditworthiness\\n\\nFinAdapt isn’t just another fintech project—it is a mission to build financial confidence, bridge opportunity gaps, \\nand bring financial dignity to millions of independent workers across the country.\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Load all text files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\",            # directory path\n",
    "    glob=\"**/*.txt\",                # pattern to match .txt files\n",
    "    loader_cls=TextLoader,          # loader to use for each file\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "\n",
    "print(\"Loaded documents:\")\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cd15867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDF documents loaded: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "# Choose one PDF loader\n",
    "pdf_loader_cls = PyMuPDFLoader  # or PyPDFLoader\n",
    "\n",
    "# Directory containing PDF files\n",
    "pdf_directory = \"../data/pdf\"\n",
    "\n",
    "# Ensure directory exists\n",
    "if not os.path.exists(pdf_directory):\n",
    "    os.makedirs(pdf_directory)\n",
    "\n",
    "# Load all PDFs\n",
    "documents = []\n",
    "\n",
    "for root, dirs, files in os.walk(pdf_directory):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".pdf\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            loader = pdf_loader_cls(file_path)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "\n",
    "print(f\"Total PDF documents loaded: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae3bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bbedc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original documents: 4\n",
      "Chunked documents: 11\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content: # Anil Kapoor's 2025 Comprehensive Financial Report \n",
      " \n",
      "**Generated by FinAdapt Agentic-AI on November 27, 2025**   \n",
      "**User Profile:** Anil Kapoor (anil12@gmail.com)   \n",
      "**Occupation:** Gig Worker | Pla...\n",
      "Metadata: {'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/data.pdf', 'file_path': '../data/pdf/data.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Untitled document', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'chunk_index': 0, 'total_chunks': 3, 'chunk_type': 'text'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content: | **Net Savings** | 159,600     | 30%        | \n",
      "| **Avg Monthly Income** | 44,583 | -          | \n",
      "| **Avg Monthly Savings** | 13,300 | -          | \n",
      " \n",
      "### Loan & Investment Snapshot \n",
      "- **Active Loan:*...\n",
      "Metadata: {'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/data.pdf', 'file_path': '../data/pdf/data.pdf', 'total_pages': 4, 'format': 'PDF 1.4', 'title': 'Untitled document', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'chunk_index': 1, 'total_chunks': 3, 'chunk_type': 'text'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize the text splitter with better parameters for financial content\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # Reduced from 1500 to keep related content together\n",
    "    chunk_overlap=200,     # Increased overlap for better context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\\\. )\", \" \", \"\"],  # Added lookbehind for sentence splitting\n",
    "    keep_separator=True,   # Keep separators in the text\n",
    "    length_function=len,   # Use character count\n",
    "    is_separator_regex=True  # Enable regex for separators\n",
    ")\n",
    "\n",
    "# Enhanced chunking with metadata preservation\n",
    "def chunk_documents(documents):\n",
    "    chunked_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Preserve original metadata\n",
    "        metadata = getattr(doc, 'metadata', {}).copy()\n",
    "        \n",
    "        # Split the document\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "        \n",
    "        # Add chunk-specific metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Preserve original metadata\n",
    "            chunk.metadata.update(metadata)\n",
    "            \n",
    "            # Add chunk-specific metadata\n",
    "            chunk.metadata.update({\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "                \"chunk_type\": \"text\"  # Could be 'table', 'figure', etc. if you add that logic\n",
    "            })\n",
    "            \n",
    "            # Clean up the chunk content\n",
    "            chunk.page_content = chunk.page_content.strip()\n",
    "            \n",
    "            # Only add non-empty chunks\n",
    "            if chunk.page_content:\n",
    "                chunked_docs.append(chunk)\n",
    "    \n",
    "    return chunked_docs\n",
    "\n",
    "# Apply the chunking to your documents\n",
    "chunked_documents = chunk_documents(documents)\n",
    "\n",
    "print(f\"Original documents: {len(documents)}\")\n",
    "print(f\"Chunked documents: {len(chunked_documents)}\")\n",
    "\n",
    "# Verify chunk quality\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(chunked_documents[:2]):  # Show first 2 chunks\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(\"Content:\", chunk.page_content[:200] + \"...\" if len(chunk.page_content) > 200 else chunk.page_content)\n",
    "    print(\"Metadata:\", {k: v for k, v in chunk.metadata.items() if not k.startswith('_')})\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc55276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Optional, Union\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"\n",
    "    Document embedding manager using SentenceTransformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"all-MiniLM-L6-v2\",\n",
    "        device: Optional[str] = None,\n",
    "        cache_folder: Optional[Union[str, Path]] = None\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.cache_folder = str(cache_folder) if cache_folder else None\n",
    "        self.model = None\n",
    "        self.embedding_dimension = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self) -> None:\n",
    "        \"\"\"Load the SentenceTransformer model.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(\n",
    "                self.model_name,\n",
    "                device=self.device,\n",
    "                cache_folder=self.cache_folder\n",
    "            )\n",
    "            self.embedding_dimension = self.model.get_sentence_embedding_dimension()\n",
    "            logger.info(f\"Model loaded. Embedding dimension: {self.embedding_dimension}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_embedding(self, text: str, normalize: bool = True) -> np.ndarray:\n",
    "        \"\"\"Generate embedding for a single text.\"\"\"\n",
    "        if not text:\n",
    "            return np.zeros(self.embedding_dimension, dtype=np.float32)\n",
    "        return self.model.encode(text, convert_to_numpy=True, normalize_embeddings=normalize)\n",
    "\n",
    "    def get_embeddings(self, texts: List[str], batch_size: int = 32, show_progress_bar: bool = True, normalize: bool = True) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        if not texts:\n",
    "            return np.empty((0, self.embedding_dimension), dtype=np.float32)\n",
    "        return self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=show_progress_bar,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=normalize\n",
    "        )\n",
    "    \n",
    "    def embed_query(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Wrapper for query embedding (used by RAGRetriever).\"\"\"\n",
    "        return self.get_embedding(text)\n",
    "\n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        return self.embedding_dimension or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4de31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from typing import List, Any\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Manages document embeddings in a ChromaDB vector store\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to the vector store\"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Documents and embeddings length mismatch\")\n",
    "\n",
    "        ids, metadatas, docs_text, embeddings_list = [], [], [], []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(getattr(doc, 'metadata', {}))\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(getattr(doc, 'page_content', ''))\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            docs_text.append(getattr(doc, 'page_content', ''))\n",
    "            embeddings_list.append(embedding.tolist() if isinstance(embedding, np.ndarray) else embedding)\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=docs_text\n",
    "            )\n",
    "            print(f\"Added {len(documents)} documents. Total now: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def search(self, query_embedding, top_k: int = 5):\n",
    "        \"\"\"Search the vector store using query embedding\"\"\"\n",
    "        try:\n",
    "            # Convert numpy array to list if needed\n",
    "            if hasattr(query_embedding, 'tolist'):\n",
    "                query_embedding = query_embedding.tolist()\n",
    "\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Format results\n",
    "            formatted_results = []\n",
    "            if results and 'metadatas' in results and 'documents' in results:\n",
    "                for meta, content in zip(results['metadatas'][0], results['documents'][0]):\n",
    "                    formatted_results.append({\n",
    "                        'content': content,\n",
    "                        'metadata': meta\n",
    "                    })\n",
    "\n",
    "            return formatted_results\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28a82ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Model loaded. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429d3d5d0750431196006705151ff93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 11 chunks.\n",
      "Shape of first embedding (example): (384,)\n"
     ]
    }
   ],
   "source": [
    "# Make sure chunked_documents is already created\n",
    "# chunked_documents = [Document(page_content=\"...\", metadata={...}), ...]\n",
    "\n",
    "# Prepare text content for embeddings\n",
    "texts = [doc.page_content for doc in chunked_documents]\n",
    "\n",
    "# --- FIX: create an instance of EmbeddingManager ---\n",
    "embedding_manager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_manager.get_embeddings(texts)\n",
    "\n",
    "print(f\"Generated embeddings for {len(texts)} chunks.\")\n",
    "\n",
    "# Check example embedding\n",
    "if embeddings is not None and len(embeddings) > 0:\n",
    "    print(f\"Shape of first embedding (example): {embeddings[0].shape}\")\n",
    "else:\n",
    "    print(\"No embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c27e40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "import traceback\n",
    "\n",
    "class RAGRetriever:\n",
    "    \"\"\"Retrieve top documents from vector store given a query\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store, embedding_manager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Any]:\n",
    "        try:\n",
    "            query_embedding = self.embedding_manager.embed_query(query)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to generate query embedding: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            results = self.vector_store.search(query_embedding, top_k=top_k)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Vector store search failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7454ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading model: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Model loaded. Embedding dimension: 384\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: finadapt_docs\n",
      "Existing documents: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc99959734d4e5bb5f104fd9b371cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 11 documents. Total now: 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0622f804fd4f449d0a123ffd903777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "Content: # Anil Kapoor's 2025 Comprehensive Financial Report \n",
      " \n",
      "**Generated by FinAdapt Agentic-AI on November 27, 2025**   \n",
      "**User Profile:** Anil Kapoor (anil12@gmail.com)   \n",
      "**Occupation:** Gig Worker | Pla...\n",
      "Metadata: {'modDate': '', 'chunk_type': 'text', 'doc_index': 0, 'producer': 'Skia/PDF m144 Google Docs Renderer', 'format': 'PDF 1.4', 'page': 0, 'content_length': 989, 'chunk_index': 0, 'subject': '', 'creationDate': '', 'moddate': '', 'total_pages': 4, 'creationdate': '', 'trapped': '', 'author': '', 'creator': '', 'title': 'Untitled document', 'file_path': '../data/pdf/data.pdf', 'keywords': '', 'total_chunks': 3, 'source': '../data/pdf/data.pdf'}\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Document 2 ---\n",
      "Content: | **Net Savings** | 159,600     | 30%        | \n",
      "| **Avg Monthly Income** | 44,583 | -          | \n",
      "| **Avg Monthly Savings** | 13,300 | -          | \n",
      " \n",
      "### Loan & Investment Snapshot \n",
      "- **Active Loan:*...\n",
      "Metadata: {'total_chunks': 3, 'producer': 'Skia/PDF m144 Google Docs Renderer', 'creationdate': '', 'author': '', 'source': '../data/pdf/data.pdf', 'creationDate': '', 'title': 'Untitled document', 'modDate': '', 'trapped': '', 'total_pages': 4, 'subject': '', 'format': 'PDF 1.4', 'chunk_type': 'text', 'creator': '', 'moddate': '', 'file_path': '../data/pdf/data.pdf', 'keywords': '', 'page': 0, 'doc_index': 1, 'content_length': 984, 'chunk_index': 1}\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Document 3 ---\n",
      "Content: | **Utilities** | 6,083          | 6,183         | +100         | Over 2%             | \n",
      "| **Entertainment** | 2,875     | 2,662         | -212         | Under 7%            | \n",
      "| **Fuel**      | 5,000...\n",
      "Metadata: {'subject': '', 'chunk_type': 'text', 'file_path': '../data/pdf/data.pdf', 'moddate': '', 'content_length': 933, 'creationdate': '', 'creator': '', 'total_pages': 4, 'modDate': '', 'keywords': '', 'title': 'Untitled document', 'total_chunks': 4, 'author': '', 'trapped': '', 'page': 1, 'producer': 'Skia/PDF m144 Google Docs Renderer', 'doc_index': 3, 'chunk_index': 0, 'creationDate': '', 'source': '../data/pdf/data.pdf', 'format': 'PDF 1.4'}\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Document 4 ---\n",
      "Content: | Month     | Budget (₹) | Income (₹) | Spent (₹) | Savings (₹) | Key Insight                  | \n",
      "|-----------|------------|------------|-----------|-------------|------------------------------| \n",
      "| Ja...\n",
      "Metadata: {'keywords': '', 'modDate': '', 'source': '../data/pdf/data.pdf', 'format': 'PDF 1.4', 'producer': 'Skia/PDF m144 Google Docs Renderer', 'title': 'Untitled document', 'author': '', 'page': 1, 'total_pages': 4, 'trapped': '', 'subject': '', 'creator': '', 'chunk_index': 1, 'chunk_type': 'text', 'creationDate': '', 'doc_index': 4, 'creationdate': '', 'total_chunks': 4, 'file_path': '../data/pdf/data.pdf', 'moddate': '', 'content_length': 948}\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Document 5 ---\n",
      "Content: | 2025-01-03 | Daily Gig     | 1,200     | Daily          | Lunch rush deliveries        | \n",
      "| 2025-01-05 | Weekly Contract| 10,000    | Weekly-Contract | Weekly retainer from cafe    | \n",
      "| 2025-01-12 |...\n",
      "Metadata: {'creationDate': '', 'author': '', 'chunk_type': 'text', 'trapped': '', 'source': '../data/pdf/data.pdf', 'doc_index': 7, 'keywords': '', 'creationdate': '', 'modDate': '', 'content_length': 908, 'creator': '', 'file_path': '../data/pdf/data.pdf', 'format': 'PDF 1.4', 'chunk_index': 0, 'total_pages': 4, 'title': 'Untitled document', 'producer': 'Skia/PDF m144 Google Docs Renderer', 'subject': '', 'moddate': '', 'total_chunks': 3, 'page': 2}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Initialize embedding manager and vector store\n",
    "embedding_manager = EmbeddingManager(\"all-MiniLM-L6-v2\")\n",
    "vector_store = VectorStore(\"finadapt_docs\", \"../data/vector_store\")\n",
    "\n",
    "# 2️⃣ Prepare documents (must exist)\n",
    "# Example: chunked_documents = [{'page_content': \"text here\", 'metadata': {...}}, ...]\n",
    "document_contents = [doc.page_content for doc in chunked_documents]\n",
    "document_embeddings = embedding_manager.get_embeddings(document_contents)\n",
    "\n",
    "# 3️⃣ Add to vector store\n",
    "vector_store.add_documents(chunked_documents, document_embeddings)\n",
    "\n",
    "# 4️⃣ Initialize retriever\n",
    "document_retriever = RAGRetriever(vector_store, embedding_manager)\n",
    "\n",
    "# 5️⃣ Retrieve\n",
    "search_results = document_retriever.retrieve(\n",
    "    query=\"What is FinAdapt's core mission?\",\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# 6️⃣ Display results\n",
    "for idx, doc in enumerate(search_results, 1):\n",
    "    print(f\"\\n--- Document {idx} ---\")\n",
    "    print(\"Content:\", doc['content'][:200] + \"...\" if len(doc['content']) > 200 else doc['content'])\n",
    "    print(\"Metadata:\", doc['metadata'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2484ce",
   "metadata": {},
   "source": [
    "### Connetion with llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23004eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5740650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Initialize Groq LLM\n",
    "# ---------------------------\n",
    "\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq   # if using LangChain wrapper\n",
    "\n",
    "groq_api_key = \"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\"   # never hardcode real keys\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_api_key,\n",
    "    model_name=\"llama-3.3-70b-versatile\",   # valid model\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Simple RAG Function\n",
    "# ---------------------------\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Simple RAG Function (Updated)\n",
    "# ---------------------------\n",
    "\n",
    "def rag_simple(query, retriever, llm, top_k=5):\n",
    "    \"\"\"\n",
    "    Enhanced RAG function that provides detailed, explanatory answers.\n",
    "    Combines retrieved context with general knowledge for comprehensive responses.\n",
    "    \"\"\"\n",
    "    # 1️⃣ Expand query for better retrieval\n",
    "    expanded_queries = [query]\n",
    "    if any(term in query.lower() for term in [\"expense\", \"categorization\", \"financial\", \"feature\", \"how\", \"what\", \"explain\"]):\n",
    "        expanded_queries.extend([\n",
    "            f\"Detailed explanation of {query}\",\n",
    "            f\"How FinAdapt implements {query}\",\n",
    "            f\"Technical details about {query} in FinAdapt\"\n",
    "        ])\n",
    "\n",
    "    # 2️⃣ Retrieve documents with expanded queries\n",
    "    all_results = []\n",
    "    for q in expanded_queries:\n",
    "        try:\n",
    "            results = retriever.retrieve(q, top_k=top_k)\n",
    "            all_results.extend([r for r in results if r not in all_results])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # 3️⃣ Build context with source tracking\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(all_results[:top_k], 1):\n",
    "        content = doc.get('content', '').strip()\n",
    "        if content:\n",
    "            context_parts.append(f\"--- Source {i} ---\\n{content}\\n\")\n",
    "\n",
    "    context = \"\\n\".join(context_parts) if context_parts else \"No specific context found.\"\n",
    "\n",
    "    # 4️⃣ Enhanced prompt for explanatory responses\n",
    "    prompt = f\"\"\"You are a knowledgeable financial technology expert explaining FinAdapt's features.\n",
    "\n",
    "Context from documentation:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a detailed, informative response that:\n",
    "1. Directly answers the question based on the context\n",
    "2. If context is limited, supplement with general industry knowledge\n",
    "3. Explain how the feature works and its benefits\n",
    "4. Provide examples or use cases where applicable\n",
    "5. Keep the explanation clear and professional\n",
    "\n",
    "Structure your response with:\n",
    "- A clear opening statement\n",
    "- Key points in bullet format\n",
    "- A brief conclusion\n",
    "\n",
    "Detailed response:\"\"\"\n",
    "\n",
    "    # 5️⃣ Generate and process the response\n",
    "    try:\n",
    "        response = llm.invoke(input=prompt)\n",
    "        answer = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        # 6️⃣ Ensure the response is sufficiently detailed\n",
    "        if len(answer.split()) < 50:  # If response is too short\n",
    "            answer = f\"\"\"{answer}\n",
    "\n",
    "            Let me elaborate further. In financial technology platforms like FinAdapt, {query.lower().replace('?', '')} typically involves:\n",
    "\n",
    "            - Advanced algorithms that analyze transaction patterns\n",
    "            - Machine learning models that improve over time\n",
    "            - User-friendly interfaces for easy management\n",
    "            - Integration with banking and payment systems\n",
    "\n",
    "            This comprehensive approach ensures users get maximum value from the platform's features.\"\"\"\n",
    "        \n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback with general knowledge\n",
    "        return f\"\"\"I'll provide a detailed explanation based on general knowledge of financial platforms:\n",
    "\n",
    "        Regarding {query}, modern financial platforms like FinAdapt typically offer:\n",
    "\n",
    "        - Intelligent systems that automatically categorize transactions\n",
    "        - Machine learning algorithms that learn from user behavior\n",
    "        - Customizable categories and rules for personalization\n",
    "        - Detailed reporting and analytics\n",
    "\n",
    "        These features help users:\n",
    "        - Better understand their spending habits\n",
    "        - Save time on financial management\n",
    "        - Make more informed financial decisions\n",
    "        - Achieve their financial goals more effectively\n",
    "\n",
    "        While I don't have the specific implementation details for FinAdapt, this reflects industry standards for such features in leading financial platforms.\"\"\"\n",
    "\n",
    "# Example usage:\n",
    "# answer = rag_simple(\"How does expense categorization work in FinAdapt?\", document_retriever, llm)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9224bf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd51682f6fe442fc88b15212349103c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809f8a2fc61a46589f00da69c57f1aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd09005bf37a47219fc4951c5b11c700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da1ff7bff93410c84d581b56cdf407a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine Anil Kapoor's total income and expenses for January 2025, we can refer to the comprehensive financial report generated by FinAdapt Agentic-AI. According to the report, the total income and expenses for January 2025 are as follows:\n",
      "\n",
      "The total income for January 2025 is ₹42,000, and the total expenses are ₹26,850, resulting in savings of ₹15,150. \n",
      "\n",
      "Here are the key points related to Anil Kapoor's financial data for January 2025:\n",
      "* Total income: ₹42,000\n",
      "* Total expenses: ₹26,850\n",
      "* Savings: ₹15,150\n",
      "* Key insight: Fuel nearing budget—monitor, indicating that Anil Kapoor should keep a close eye on his fuel expenses to avoid exceeding his budget.\n",
      "* The data is sourced from FinAdapt's dashboard, which provides a comprehensive overview of Anil Kapoor's financial transactions, including income, expenses, and savings.\n",
      "\n",
      "FinAdapt's feature of tracking income and expenses works by aggregating data from various sources, including daily gigs, weekly contracts, and other financial transactions. This data is then categorized and analyzed to provide insights into Anil Kapoor's financial habits and trends. The benefits of this feature include:\n",
      "* Accurate tracking of income and expenses\n",
      "* Identification of areas for cost savings\n",
      "* Provision of key insights to inform financial decisions\n",
      "* Enablement of cash flow forecasting and budgeting\n",
      "\n",
      "For example, in January 2025, Anil Kapoor's income from daily gigs and weekly contracts is broken down into specific transactions, such as the ₹750 earned from an evening delivery shift on January 2, 2025. This level of detail allows Anil Kapoor to understand his income streams and make informed decisions about his finances.\n",
      "\n",
      "In conclusion, FinAdapt's comprehensive financial report provides a detailed overview of Anil Kapoor's income and expenses for January 2025, highlighting areas for cost savings and providing key insights to inform his financial decisions. By leveraging FinAdapt's features, Anil Kapoor can optimize his financial management, achieve his savings goals, and make data-driven decisions to drive his financial well-being.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\n",
    "    \"What is the total income and expenses for Anil Kapoor in January 2025?\",\n",
    "    document_retriever,  # Your retriever instance\n",
    "    llm,\n",
    "    top_k=5\n",
    ")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
